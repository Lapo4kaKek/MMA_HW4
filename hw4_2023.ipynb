{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSE 2021: Mathematical Methods for Data Analysis\n",
    "\n",
    "## Homework 4\n",
    "\n",
    "**Warning 1**: You have 2 weeks for this assignemnt.  **it is better to start early (!)**\n",
    "\n",
    "**Warning 2**: it is critical to describe and explain what you are doing and why, use markdown cells\n",
    "\n",
    "\n",
    "### Contents\n",
    "\n",
    "#### Decision Trees - 7 points\n",
    "* [Task 1](#task1) (0.5 points)\n",
    "* [Task 2](#task2) (0.5 points)\n",
    "* [Task 3](#task3) (2 points)\n",
    "* [Task 4](#task4) (0.5 points)\n",
    "* [Task 5](#task5) (0.5 points)\n",
    "* [Task 6](#task6) (2 points)\n",
    "* [Task 7](#task7) (0.5 points)\n",
    "* [Task 8](#task8) (0.5 points)\n",
    "\n",
    "#### Ensembles - 3 points\n",
    "* [Task 1](#task2_1) (1 point)\n",
    "* [Task 2](#task2_2) (0.7 points)\n",
    "* [Task 3](#task2_3) (0.5 points)\n",
    "* [Task 4](#task2_4) (0.7 points)\n",
    "* [Task 5](#task2_5) (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.727636Z",
     "start_time": "2023-11-09T17:46:40.045013Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (11, 5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will be implementing decision tree for the regression by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task1\"></a> (0.5 points)\n",
    "\n",
    "Here you should implement the function `H()` which calculates impurity criterion. We will be training regression tree, and will take mean absolute deviation as impurity criterion.\n",
    "\n",
    "* You cannot use loops\n",
    "* If `y` is empty, the function should return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.732847Z",
     "start_time": "2023-11-09T17:46:40.729241Z"
    }
   },
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    \"\"\"\n",
    "    Calculate impurity criterion\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array\n",
    "        array of objects target values in the node\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    H(R) : float\n",
    "        Impurity in the node (measuread by variance)\n",
    "    \"\"\"\n",
    "    if len(y) == 0: return 0;\n",
    "    return np.mean(np.abs(y - np.mean(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.875430Z",
     "start_time": "2023-11-09T17:46:40.737694Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the function\n",
    "assert np.allclose(H(np.array([4, 2, 2, 2])), 0.75)\n",
    "assert np.allclose(H(np.array([])), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2\"></a>  (0.5 points)\n",
    "\n",
    "To find the best split in the node we need to calculate the cost function. Denote: \n",
    "- `R` all the object in the node\n",
    "- `j` index of the feature selected for the split\n",
    "- `t` threshold\n",
    "- `R_l` and `R_r` objects in the left and right child nodes correspondingly\n",
    "\n",
    "We get the following cost function:\n",
    "\n",
    "$$\n",
    "Q(R, j, t) =\\frac{|R_\\ell|}{|R|}H(R_\\ell) + \\frac{|R_r|}{|R|}H(R_r) \\to \\min_{j, t},\n",
    "$$\n",
    "\n",
    "Implement the function `Q`, which should calculate value of the cost function for a given feature and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.877419Z",
     "start_time": "2023-11-09T17:46:40.877407Z"
    }
   },
   "outputs": [],
   "source": [
    "def Q(X, y, j, t):\n",
    "    \"\"\"\n",
    "    Calculate cost function\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        array of objects in the node \n",
    "    y : ndarray\n",
    "        array of target values in the node \n",
    "    j : int\n",
    "        feature index (column in X)\n",
    "    t : float\n",
    "        threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : float\n",
    "        Value of the cost function\n",
    "    \"\"\"   \n",
    "    # YOUR CODE HERE\n",
    "     # Разделение данных на две группы на основе порога t\n",
    "    left_indices = X[:, j] <= t\n",
    "    right_indices = ~left_indices\n",
    "\n",
    "    y_left = y[left_indices]\n",
    "    y_right = y[right_indices]\n",
    "\n",
    "    # Вычисление импурити для каждой группы\n",
    "    H_left = H(y_left)\n",
    "    H_right = H(y_right)\n",
    "\n",
    "    # Вычисление функции стоимости Q\n",
    "    Q_value = (len(y_left) / len(y)) * H_left + (len(y_right) / len(y)) * H_right\n",
    "    return Q_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task3\"></a>  (2 points)\n",
    "\n",
    "Now, let's implement `MyDecisionTreeRegressor` class. More specifically, you need to implement the following methods:\n",
    "\n",
    "- `best_split`\n",
    "- `grow_tree`\n",
    "- `get_prediction`\n",
    "\n",
    "Also, please add `min_samples_leaf` parameter to your class\n",
    "\n",
    "Read docstrings for more details. Do not forget to use function `Q` implemented above, when finding the `best_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.878647Z",
     "start_time": "2023-11-09T17:46:40.878635Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    \"\"\"\n",
    "    Class for a decision tree node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    right : Node() or None\n",
    "        Right child\n",
    "    right : Node() or None\n",
    "        Left child\n",
    "    threshold: float\n",
    "        \n",
    "    column: int\n",
    "        \n",
    "    depth: int\n",
    "        \n",
    "    prediction: float\n",
    "        prediction of the target value in the node \n",
    "        (average values calculated on a train dataset)\n",
    "    is_terminal:bool\n",
    "        indicates whether it is a terminal node (leaf) or not\n",
    "    \"\"\"    \n",
    "    def __init__(self):        \n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.threshold = None\n",
    "        self.column = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "        self.prediction = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.is_terminal:\n",
    "            node_desc = 'Pred: {:.2f}'.format(self.prediction)\n",
    "        else:\n",
    "            node_desc = 'Col {}, t {:.2f}, Pred: {:.2f}'. \\\n",
    "            format(self.column, self.threshold, self.prediction)\n",
    "        return node_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.879812Z",
     "start_time": "2023-11-09T17:46:40.879801Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "\n",
    "class MyDecisionTreeRegressor(RegressorMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Class for a Decision Tree Regressor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_depth : int\n",
    "        Max depth of a decision tree.\n",
    "    min_samples_split : int\n",
    "        Minimal number of samples (objects) in a node to make a split.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best split in terms of Q of data in a given decision tree node.\n",
    "        Try all features and thresholds.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects in the parent node\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            1D array with the object labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        best_split_column : int\n",
    "            Index of the best split column\n",
    "        best_threshold : float\n",
    "            The best split condition.\n",
    "        X_left : ndarray, shape (n_objects_l, n_features)\n",
    "            Objects in the left child\n",
    "        y_left : ndarray, shape (n_objects_l, )\n",
    "            Objects labels in the left child.\n",
    "        X_right : ndarray, shape (n_objects_r, n_features)\n",
    "            Objects in the right child\n",
    "        y_right : ndarray, shape (n_objects_r, )\n",
    "            Objects labels in the right child.\n",
    "        \"\"\"\n",
    "        # To store best split parameters\n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        # without splitting\n",
    "        best_split_column = None\n",
    "        best_threshold = None\n",
    "        # without splitting\n",
    "        best_cost = H(y)\n",
    "        for split_column in range(X.shape[1]):\n",
    "            column = X[:, split_column]\n",
    "        for threshold in column:\n",
    "            cost = Q(X, y, split_column, threshold)\n",
    "        if cost < best_cost:\n",
    "            best_cost = cost\n",
    "        best_split_column = split_column\n",
    "        best_threshold = threshold\n",
    "        if best_split_column == None:\n",
    "            return None, None, None, None, None, None\n",
    "        bools = X[:, best_split_column] < best_threshold\n",
    "        return best_split_column, best_threshold, X[bools, :], y[bools], X[~bools, :],y[~bools]\n",
    "\n",
    "    def is_terminal(self, node, y):\n",
    "        \"\"\"\n",
    "        Check terminality conditions based on `max_depth`,\n",
    "        `min_samples_split` parameters for a given node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node,\n",
    "\n",
    "        y : ndarray, shape (n_objects, )\n",
    "            Object labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Is_termial : bool\n",
    "            If True, node is terminal\n",
    "        \"\"\"\n",
    "        if node.depth >= self.max_depth:\n",
    "            return True\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def grow_tree(self, node, X, y):\n",
    "        \"\"\"\n",
    "        Reccurently grow the tree from the `node` using a `X` and `y` as a dataset:\n",
    "         - check terminality conditions\n",
    "         - find best split if node is not terminal\n",
    "         - add child nodes to the node\n",
    "         - call the function recursively for the added child nodes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        X : ndarray, shape (n_objects, n_features)\n",
    "            Objects\n",
    "        y : ndarray, shape (n_objects)\n",
    "            Labels\n",
    "        \"\"\"\n",
    "\n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        if self.is_terminal(node, y):\n",
    "            node.is_terminal = True\n",
    "        return\n",
    "        best_split_column, best_threshold, X_left, y_left, X_right, y_right = self.best_split(X, y)\n",
    "        if best_split_column == None:\n",
    "            node.is_terminal = True\n",
    "        return\n",
    "        node.column = best_split_column\n",
    "        node.threshold = best_threshold\n",
    "        node.left = Node(node.depth + 1)\n",
    "        node.right = Node(node.depth + 1)\n",
    "        node.left.prediction = np.mean(y_left)\n",
    "        node.right.prediction = np.mean(y_right)\n",
    "        self.grow_tree(node.left, X_left, y_left)\n",
    "        self.grow_tree(node.right, X_right, y_right)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Decision Tree Regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : ndarray, shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y, accept_sparse=False)\n",
    "        self.is_fitted_ = True\n",
    "\n",
    "        # Initialize the tree (root node)\n",
    "        self.tree_ = Node()\n",
    "        self.tree_.depth = 1\n",
    "        self.tree_.prediction = np.mean(y)\n",
    "\n",
    "        # Grow the tree\n",
    "        self.grow_tree(self.tree_, X, y)\n",
    "        return self\n",
    "\n",
    "    def get_prediction(self, node, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Get prediction for an object `x`\n",
    "            - Return prediction of the `node` if it is terminal\n",
    "            - Otherwise, recursively call the function to get\n",
    "            predictions of the proper child\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        node : Node() object\n",
    "            Current node of the decision tree.\n",
    "        x : ndarray, shape (n_features,)\n",
    "            Array of feature values of one object.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : float\n",
    "            Prediction for an object x\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        if node.is_terminal:\n",
    "            return node.prediction\n",
    "        if x[node.column] < node.threshold:\n",
    "            return self.get_prediction(node.left, x)\n",
    "        return self.get_prediction(node.right, x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Get prediction for each object in X\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Returns predictions.\n",
    "        \"\"\"\n",
    "        # Check input and that `fit` had been called\n",
    "        X = check_array(X, accept_sparse=False)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "\n",
    "        # Get predictions\n",
    "        y_predicted = []\n",
    "        for x in X:\n",
    "            y_curr = self.get_prediction(self.tree_, x)\n",
    "            y_predicted.append(y_curr)\n",
    "        return np.array(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check yourself\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator_checks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_estimator\n\u001b[1;32m      3\u001b[0m check_estimator(MyDecisionTreeRegressor())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/estimator_checks.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_args\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_raise_message\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_testing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_array_equal\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/_testing.py:400\u001b[0m\n\u001b[1;32m    395\u001b[0m skip_travis \u001b[38;5;241m=\u001b[39m pytest\u001b[38;5;241m.\u001b[39mmark\u001b[38;5;241m.\u001b[39mskipif(\n\u001b[1;32m    396\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAVIS\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip on travis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m )\n\u001b[1;32m    398\u001b[0m fails_if_pypy \u001b[38;5;241m=\u001b[39m pytest\u001b[38;5;241m.\u001b[39mmark\u001b[38;5;241m.\u001b[39mxfail(IS_PYPY, reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot compatible with PyPy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    399\u001b[0m fails_if_unstable_openblas \u001b[38;5;241m=\u001b[39m pytest\u001b[38;5;241m.\u001b[39mmark\u001b[38;5;241m.\u001b[39mxfail(\n\u001b[0;32m--> 400\u001b[0m     \u001b[43m_in_unstable_openblas_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    401\u001b[0m     reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS is unstable for this configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    402\u001b[0m )\n\u001b[1;32m    403\u001b[0m skip_if_no_parallel \u001b[38;5;241m=\u001b[39m pytest\u001b[38;5;241m.\u001b[39mmark\u001b[38;5;241m.\u001b[39mskipif(\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m joblib\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39mmp, reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib is in serial mode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    405\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m#  Decorator for tests involving both BLAS calls and multiprocessing.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m#  Under POSIX (e.g. Linux or OSX), using multiprocessing in conjunction\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m#  errors on interactively defined functions. It therefore not enabled by\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m#  default.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/__init__.py:90\u001b[0m, in \u001b[0;36m_in_unstable_openblas_configuration\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m modules_info \u001b[38;5;241m=\u001b[39m \u001b[43mthreadpool_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m open_blas_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_api\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m modules_info)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m open_blas_used:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/fixes.py:150\u001b[0m, in \u001b[0;36mthreadpool_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreadpoolctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreadpool_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:124\u001b[0m, in \u001b[0;36mthreadpool_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@_format_docstring\u001b[39m(USER_APIS\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(_ALL_USER_APIS),\n\u001b[1;32m    108\u001b[0m                    INTERNAL_APIS\u001b[38;5;241m=\u001b[39m_ALL_INTERNAL_APIS)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mthreadpool_info\u001b[39m():\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the maximal number of threads for each detected library.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Return a list with all the supported modules that have been found. Each\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    In addition, each module may contain internal_api specific entries.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ThreadpoolInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ALL_USER_APIS\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtodicts()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:371\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_modules_with_dyld\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:428\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_module_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[1;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[0;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[0;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# check yourself\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "check_estimator(MyDecisionTreeRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task4\"></a>  (0.5 points)\n",
    "\n",
    "Load boston dataset and split it on the train ($75\\%$) and test ($25\\%$). Fit Decision Tree of **depth 1, 3, 5** and make the following plots for every case:\n",
    "\n",
    "- Scatter plot of the traning points (selected for split feature on the x-axis, target variable on the y-axis)\n",
    "- Fitted model (tree visualization)\n",
    "\n",
    "Compare `MAE` on train and test. Have trees overfitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.882171Z",
     "start_time": "2023-11-09T17:46:40.882159Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task5\"></a>  (0.5 points)\n",
    "\n",
    "Keep working with boston dataset. \n",
    "- Use `GridSearchCV` to find the best hyperparameters among [`max_depth`, `min_samples_leaf`] on 5-Fold cross-validation\n",
    "- Train the model with the best set of hyperparameters on the whole train dataset. \n",
    "- Report `MAE` on test dataset and hyperparameters of the best estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.884202Z",
     "start_time": "2023-11-09T17:46:40.884125Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 <a id=\"task6\"></a>  (2 points)\n",
    "\n",
    "Recall definition of bias and variance:\n",
    "$$\n",
    "\\text{Bias}^2 = \\mathbb{E}_{p(x, y)} \\left[  (f(x) - \\mathbb{E}_{\\mathbb{X}}a_{\\mathbb{X}}(x))^2 \\right] \\\\\n",
    "\\text{Variance} = \\mathbb{E}_{p(x, y)} \\left[  \\mathbb{V}_{\\mathbb{X}}( a_{\\mathbb{X}}(x))  \\right]\n",
    "$$\n",
    "\n",
    "We wil now use the following algorithm to estimate bias and variance:\n",
    "\n",
    "1. Use bootsrap to create `n_iter` samples from the original dataset: $X_1, \\dots, X_{n_iter}$\n",
    "2. For each bootstrapped sample define out-of-bag (OOB) sample $Z_1, \\dots, Z_{n_iter}$, which contain all the observations, which did not appear in the corresponding boostraped sample\n",
    "3. Fit the model on $X_i$s and compute predictions on $Z_i$s\n",
    "4. For a given *object* $n$:\n",
    "     - bias^2: squared difference between true value $y_n$ and average prediction (average over the algorithms, for which $n$ was in OOB)\n",
    "     - variance: variance of the prediction (predictions of the algorithms, for which $n$ was in OOB)\n",
    "5. Average bias^2 and variance over all the points\n",
    "    \n",
    "**Implement `get_bias_variance` function, using the algorithm above**\n",
    "\n",
    "*Note:*  You can only use 1 loop (for bootsrap iterations). All other operations should be vectorized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.887222Z",
     "start_time": "2023-11-09T17:46:40.887195Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bias_variance(estimator, x, y, n_iter):\n",
    "    \"\"\" \n",
    "    Calculate bias and variance of the `estimator`. \n",
    "    Using a given dataset and bootstrap with `n_iter` samples. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    y : ndarray, shape (n_samples, n_features)\n",
    "        The input samples.\n",
    "    n_iter: int\n",
    "        Number of samples in \n",
    "    Returns\n",
    "    -------\n",
    "    bias2 : float, \n",
    "        Estiamted squared bias\n",
    "    variance : float, \n",
    "        Estiamted variance\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.888721Z",
     "start_time": "2023-11-09T17:46:40.888706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "estimator = MyDecisionTreeRegressor(max_depth=8, min_samples_split=15)\n",
    "\n",
    "get_bias_variance(estimator, X_train.values, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7 <a id=\"task7\"></a>  (0.5 points)\n",
    "\n",
    "Compute bias and variance for the trees with different min_samples_split. Plot how bias and variance change as min_samples_split increases. \n",
    "\n",
    "Comment on what you observe, how does your result correspond to theory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.889695Z",
     "start_time": "2023-11-09T17:46:40.889683Z"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 <a id=\"task8\"></a>  (0.5 points)\n",
    "\n",
    "Let's try to reduce variance with bagging. Use `sklearn.ensemble.BaggingRegressor` to get an ensemble and compute its bias and variance. \n",
    "\n",
    "Answer the following questions:\n",
    " - How bagging should affect bias and variance in theory?\n",
    " - How bias and variance change (if they change) compared to an individual tree in you experiments? \n",
    " - Do your results align with the theory? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:46:40.891228Z",
     "start_time": "2023-11-09T17:46:40.891211Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. More Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will be working with [Billionaires Statistics Dataset](https://www.kaggle.com/datasets/nelgiriyewithana/billionaires-statistics-dataset) to solve a classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T17:47:35.163040Z",
     "start_time": "2023-11-09T17:47:35.075668Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('Billionaires Statistics Dataset.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['selfMade'])\n",
    "X = df.drop('selfMade', axis=1)\n",
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 <a id=\"task2_1\"></a> (1 point)\n",
    "\n",
    "Let's start with data preprocessing. \n",
    "\n",
    "0. Drop columns, which are not usefull (e.g. a lot of missing values). Motivate your choice. \n",
    "1. Split dataset into train and test\n",
    "2. You've probably noticed that we have both categorical and numerical columns. Here is what you need to do with them:\n",
    "    - Categorical: Fill missing values and apply one-hot-encoding (if there are many unique values in a column, you can group them by meaning)\n",
    "    - Numeric: Fill missing values\n",
    "    \n",
    "Use `ColumnTranformer` to define a single transformer for all the columns in the dataset. It takes as input a list of tuples\n",
    "\n",
    "```\n",
    "ColumnTransformer([\n",
    "    ('name1', transform1, column_names1),\n",
    "    ('name2', transform2, column_names2)\n",
    "])\n",
    "```\n",
    "\n",
    "Pay attention to an argument `remainder='passthrough'`. [Here](https://scikit-learn.org/stable/modules/compose.html#column-transformer) you can find some examples of how to use column transformer. \n",
    "    \n",
    "Since we want to apply 2 transformations to categorical feature, it is very convenient to combine them into a `Pipeline`:\n",
    "\n",
    "```\n",
    "double_tranform = make_pipeline(\n",
    "                        transform_1,\n",
    "                        transform_2\n",
    "                        )\n",
    "```\n",
    "\n",
    "P.S. Choose your favourite way to fill missing values. \n",
    "\n",
    "*Hint* Categorical column usually have `dtype = 'object'`. This may help to obtain list of categorical and numerical columns on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# define column_transformer \n",
    "\n",
    "# Transform the data\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "X_test = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 <a id=\"task2_2\"></a> (0.7 points)\n",
    "\n",
    "Fit and compare 5 different models (use sklearn): Gradient Boosting, Random Forest, Decision Tree, SVM, Logitics Regression\n",
    "    \n",
    "* Choose one classification metric and justify your choice .\n",
    "* Compare the models using score on cross validation. Mind the class balance when choosing the cross validation. (You can read more about different CV strategies [here](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold))\n",
    "* Which model has the best performance? Which models overfit or underfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 <a id=\"task2_3\"></a> (0.5 points)\n",
    "\n",
    "More Gradient Boosting. You will have to take one of the three popular boosting implementations (xgboost, lightgbm, catboost). Select hyperparameters (number of trees, learning rate, depth) on cross-validation and compare with the methods from the previous task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 <a id=\"task2_4\"></a> (0.7 points)\n",
    "\n",
    "Now let's train more fancy ensembles:\n",
    "\n",
    "* Bagging with decision trees as base estimators\n",
    "* Bagging with gradient boosting (with large amount of trees, >100) as base estimators\n",
    "* [Voting classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier) \n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Logistic Regression as a final model\n",
    "* [Stacking Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier) with Gradeint Boosting as a final model\n",
    "\n",
    "\n",
    "If not stated in the task, feel free to tune / choose hyperparameters and base models.\n",
    "\n",
    "Answer the questions:\n",
    "* Which model has the best performance?\n",
    "* Does bagging reduce overfiting of the gradient boosting with large amount of trees? \n",
    "* What is the difference between voting and staking? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```your comments here```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 <a id=\"task2_5\"></a> (0.1 points)\n",
    "\n",
    "Report the test score for the best model, that you were able to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
